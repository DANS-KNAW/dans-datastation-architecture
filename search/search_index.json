{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About this site \u00b6 This documentation site discusses the technical details of the core software-based services of DANS, the Dutch national centre of expertise and repository for research data. It is intended for developers and system administrators, both within DANS and outside. For more general information on DANS, its mission and services, see the DANS website . The documentation discusses the following topics: Architecture . The core services of DANS are centered around the concept of a Data Station . See Core Services for more information about what services are available. Each Data Station has basically the same structure. To read more about the architecture of a Data Station, see the section about the Data Station architecture . This page is also a jumping off point for more detailed information about the various components of the system. Configurations . The basic Data Station configuration has a couple of variations such as DataverseNL , Vault as a Service Interfaces . The Data Stations have a number of interfaces, both internal and external. For more information, see the sections about internal interfaces and about external interfaces . Development . The Data Stations are developed by the DANS Core Systems Team. For more information about the development environment, see the section about development . EASY migration The Data Stations are the successor of the legacy repository system EASY , which was in use from 2007 until 2024. Some parts of this documentation still contain references to EASY, but these will be removed as we are wrapping up the migration process.","title":"About"},{"location":"#about-this-site","text":"This documentation site discusses the technical details of the core software-based services of DANS, the Dutch national centre of expertise and repository for research data. It is intended for developers and system administrators, both within DANS and outside. For more general information on DANS, its mission and services, see the DANS website . The documentation discusses the following topics: Architecture . The core services of DANS are centered around the concept of a Data Station . See Core Services for more information about what services are available. Each Data Station has basically the same structure. To read more about the architecture of a Data Station, see the section about the Data Station architecture . This page is also a jumping off point for more detailed information about the various components of the system. Configurations . The basic Data Station configuration has a couple of variations such as DataverseNL , Vault as a Service Interfaces . The Data Stations have a number of interfaces, both internal and external. For more information, see the sections about internal interfaces and about external interfaces . Development . The Data Stations are developed by the DANS Core Systems Team. For more information about the development environment, see the section about development . EASY migration The Data Stations are the successor of the legacy repository system EASY , which was in use from 2007 until 2024. Some parts of this documentation still contain references to EASY, but these will be removed as we are wrapping up the migration process.","title":"About this site"},{"location":"components/","text":"Components \u00b6 The DANS Data Stations architecture is implemented using a number of internal and external components. The following pages give a description per component, with links to the relevant documentation and code repositories. For an explanation of how they fit together, see the Data Station architecture . Dataverse DANS Microservices DANS Libraries Skosmos NBN Resolver SURF Data Archive","title":"Overview"},{"location":"components/#components","text":"The DANS Data Stations architecture is implemented using a number of internal and external components. The following pages give a description per component, with links to the relevant documentation and code repositories. For an explanation of how they fit together, see the Data Station architecture . Dataverse DANS Microservices DANS Libraries Skosmos NBN Resolver SURF Data Archive","title":"Components"},{"location":"configurations/","text":"Configurations \u00b6 The Data Station architecture gives a view of the components of one Data Station. Compare it to a slice in the core services diagram , including the Data Vault at the core. As stated before, there are two slices in this diagram that are not Data Stations, but can be viewed as variations on\u2014or different configurations of\u2014the Data Station architecture. Apart from these, there is a temporary configuration that will be used until the Data Vault is fully operational. Details about the various configurations can be found in the following sections: DataverseNL Vault as a Service Temporary Vault","title":"Overview"},{"location":"configurations/#configurations","text":"The Data Station architecture gives a view of the components of one Data Station. Compare it to a slice in the core services diagram , including the Data Vault at the core. As stated before, there are two slices in this diagram that are not Data Stations, but can be viewed as variations on\u2014or different configurations of\u2014the Data Station architecture. Apart from these, there is a temporary configuration that will be used until the Data Vault is fully operational. Details about the various configurations can be found in the following sections: DataverseNL Vault as a Service Temporary Vault","title":"Configurations"},{"location":"core-services/","text":"Core Services \u00b6 Overview \u00b6 The DANS Core Services are centered around the concept of a Data Station . A Data Station is a repository system that is used for depositing, curating and disseminating datasets, as well as creating long-term preservation copies of those datasets. These long-term preservation copies are stored in the DANS Data Vault. The following diagram gives a high-level overview: Data Stations \u00b6 Each Data Station targets a part of the scientific research community. There is a Data Station for each of the following community domains: Archaeology Social Sciences and Humanities Life Sciences Physical and Technical Sciences The Data Stations use Dataverse as their repository system. Dataverse is an open source repository system developed by Harvard University. The Data Stations create a long-term preservation copy of each dataset in the DANS Data Vault. Other services \u00b6 The Data Stations are trusted repositories (some are still in the process of being certified). They are displayed as blue slices in the diagram. The grey slices are not Data Stations , as they are not in themselves full trusted repositories. In the technical architecture, however, they are described as variations on the Data Station architecture , as they are built using mostly the same components . DataverseNL \u00b6 DataverseNL is a Dataverse installation that offers deposit and dissemination services. Datasets stored in DataverseNL are also preserved in the DANS Data Vault. However, curation of the datasets is the responsibility of the DataverseNL customer. Vault as a Service \u00b6 Vault as a Service offers an interface for automated deposit of datasets directly into the DANS Data Vault. This service can be used as a building block in a customer's own archival workflow. Interfaces \u00b6 The services have interfaces for human as well as machine use. This is represented in the diagram by the people and computer icons. See for more information under External interfaces .","title":"Core Services"},{"location":"core-services/#core-services","text":"","title":"Core Services"},{"location":"core-services/#overview","text":"The DANS Core Services are centered around the concept of a Data Station . A Data Station is a repository system that is used for depositing, curating and disseminating datasets, as well as creating long-term preservation copies of those datasets. These long-term preservation copies are stored in the DANS Data Vault. The following diagram gives a high-level overview:","title":"Overview"},{"location":"core-services/#data-stations","text":"Each Data Station targets a part of the scientific research community. There is a Data Station for each of the following community domains: Archaeology Social Sciences and Humanities Life Sciences Physical and Technical Sciences The Data Stations use Dataverse as their repository system. Dataverse is an open source repository system developed by Harvard University. The Data Stations create a long-term preservation copy of each dataset in the DANS Data Vault.","title":"Data Stations"},{"location":"core-services/#other-services","text":"The Data Stations are trusted repositories (some are still in the process of being certified). They are displayed as blue slices in the diagram. The grey slices are not Data Stations , as they are not in themselves full trusted repositories. In the technical architecture, however, they are described as variations on the Data Station architecture , as they are built using mostly the same components .","title":"Other services"},{"location":"core-services/#dataversenl","text":"DataverseNL is a Dataverse installation that offers deposit and dissemination services. Datasets stored in DataverseNL are also preserved in the DANS Data Vault. However, curation of the datasets is the responsibility of the DataverseNL customer.","title":"DataverseNL"},{"location":"core-services/#vault-as-a-service","text":"Vault as a Service offers an interface for automated deposit of datasets directly into the DANS Data Vault. This service can be used as a building block in a customer's own archival workflow.","title":"Vault as a Service"},{"location":"core-services/#interfaces","text":"The services have interfaces for human as well as machine use. This is represented in the diagram by the people and computer icons. See for more information under External interfaces .","title":"Interfaces"},{"location":"dans-libraries/","text":"DANS Libraries \u00b6 DANS makes extensive use of open source libraries. We also provide some of our own. dans-bagit-lib \u00b6 A fork from the Java BagIt library that is used to create, validate and read BagIt packages. Docs Code dans-bagit-lib https://github.com/DANS-KNAW/dans-bagit-lib dans-bagpack-lib \u00b6 Classes for working with BagPack packages. Docs Code dans-bagpack-lib https://github.com/DANS-KNAW/dans-bagpack-lib dans-converter-lib \u00b6 DANS Jackson and JPA converters. Docs Code dans-converter-lib https://github.com/DANS-KNAW/dans-converter-lib dans-dataverse-client-lib \u00b6 A client library for the Dataverse API . Docs Code dans-dataverse-client-lib https://github.com/DANS-KNAW/dans-dataverse-client-lib Code examples dans-java-utils \u00b6 A collection of utility classes for Java geared towards DANS applications. Docs Code dans-java-utils https://github.com/DANS-KNAW/dans-java-utils dans-layer-store-lib \u00b6 Implementation of a \"layer store\": a write-only store that allows for the mutation of data. Docs Code dans-layer-store-lib https://github.com/DANS-KNAW/dans-layer-store-lib dans-ocfl-java-extensions-lib \u00b6 Extension classes for the OCFL Java library . Specifically, it implements a layer store for OCFL objects. Docs Code dans-ocfl-java-extensions-lib https://github.com/DANS-KNAW/dans-ocfl-java-extensions-lib dans-validation-lib \u00b6 A library of annotations and validators. Docs Code dans-validation-lib https://github.com/DANS-KNAW/dans-validation-lib","title":"DANS Libraries"},{"location":"dans-libraries/#dans-libraries","text":"DANS makes extensive use of open source libraries. We also provide some of our own.","title":"DANS Libraries"},{"location":"dans-libraries/#dans-bagit-lib","text":"A fork from the Java BagIt library that is used to create, validate and read BagIt packages. Docs Code dans-bagit-lib https://github.com/DANS-KNAW/dans-bagit-lib","title":"dans-bagit-lib"},{"location":"dans-libraries/#dans-bagpack-lib","text":"Classes for working with BagPack packages. Docs Code dans-bagpack-lib https://github.com/DANS-KNAW/dans-bagpack-lib","title":"dans-bagpack-lib"},{"location":"dans-libraries/#dans-converter-lib","text":"DANS Jackson and JPA converters. Docs Code dans-converter-lib https://github.com/DANS-KNAW/dans-converter-lib","title":"dans-converter-lib"},{"location":"dans-libraries/#dans-dataverse-client-lib","text":"A client library for the Dataverse API . Docs Code dans-dataverse-client-lib https://github.com/DANS-KNAW/dans-dataverse-client-lib Code examples","title":"dans-dataverse-client-lib"},{"location":"dans-libraries/#dans-java-utils","text":"A collection of utility classes for Java geared towards DANS applications. Docs Code dans-java-utils https://github.com/DANS-KNAW/dans-java-utils","title":"dans-java-utils"},{"location":"dans-libraries/#dans-layer-store-lib","text":"Implementation of a \"layer store\": a write-only store that allows for the mutation of data. Docs Code dans-layer-store-lib https://github.com/DANS-KNAW/dans-layer-store-lib","title":"dans-layer-store-lib"},{"location":"dans-libraries/#dans-ocfl-java-extensions-lib","text":"Extension classes for the OCFL Java library . Specifically, it implements a layer store for OCFL objects. Docs Code dans-ocfl-java-extensions-lib https://github.com/DANS-KNAW/dans-ocfl-java-extensions-lib","title":"dans-ocfl-java-extensions-lib"},{"location":"dans-libraries/#dans-validation-lib","text":"A library of annotations and validators. Docs Code dans-validation-lib https://github.com/DANS-KNAW/dans-validation-lib","title":"dans-validation-lib"},{"location":"dans-microservices/","text":"DANS Microservices & Tools \u00b6 Several of the components in the architecture are implemented as microservices and developed by DANS. Microservice projects often have an associated API project, containing the OpenAPI specification as well as a Command Line Interface (CLI) project that provides a command line tool for interacting with the microservice. API versioning The head of the API project is not necessarily the version implemented by the microservice, but rather the latest version of the OpenAPI specification. dd-confirm-archiving \u00b6 Monitors dataset version exports that are being archived by dd-data-vault and registers the archival timestamp in the Vault Catalog once the export is archived in the DANS Data Vault. Docs Code dd-confirm-archiving https://github.com/DANS-KNAW/dd-confirm-archiving dd-confirm-archiving-api https://github.com/DANS-KNAW/dd-confirm-archiving-api dd-data-vault \u00b6 Service that manages a Data Vault Storage Root and provides a REST API for interacting with it. Docs Code dd-data-vault https://github.com/DANS-KNAW/dd-data-vault dd-data-vault-api https://github.com/DANS-KNAW/dd-data-vault-api dd-data-vault-cli https://github.com/DANS-KNAW/dd-data-vault-cli dd-dataverse-authenticator \u00b6 A proxy that authenticates clients on behalf of Dataverse, using the basic auth protocol or a Dataverse API token. It is used by dd-sword2 to authenticate its clients by their Dataverse account credentials. Docs Code dd-dataverse-authenticator https://github.com/DANS-KNAW/dd-dataverse-authenticator dd-dataverse-cli \u00b6 Command line tool for interacting with the Dataverse service. It uses dans-dataverse-client-lib to communicate with the Dataverse API. Docs Code dd-dataverse-cli https://github.com/DANS-KNAW/dd-dataverse-cli Replacement for dans-datastation-tools This will replace most of the old dans-datastation-tools command line tools, which are now deprecated. dd-dataverse-ingest \u00b6 Service for ingesting deposit directories into Dataverse. Docs Code dd-dataverse-ingest https://github.com/DANS-KNAW/dd-dataverse-ingest dd-dataverse-ingest-api https://github.com/DANS-KNAW/dd-dataverse-ingest-api dd-dataverse-ingest-cli https://github.com/DANS-KNAW/dd-dataverse-ingest-cli dd-gmh \u00b6 Command line tool for interacting with the BRI-GMH resolver to resolve a DANS Persistent Identifier (NBN) to a URL. Docs Code dd-gmh-api https://github.com/DANS-KNAW/dd-gmh-api In code https://persistent-identifier.nl/gmh-registration-service/api/v1/openapi.yaml dd-gmh-cli https://github.com/DANS-KNAW/dd-gmh-cli OpenAPI specification The dd-gmh-api project contains a copy of the OpenAPI specification to be used in the build process of the dd-gmh-cli project. Note, however, that the GMH used to be maintained by DANS, but is currently maintained and hosted by Seecr on behalf of the Koninklijke Bibliotheek (KB). The authoritative OpenAPI specification is published at https://persistent-identifier.nl/gmh-registration-service/api/v1/openapi.yaml . dd-manage-deposit \u00b6 Service that manages and maintains information about deposits in a deposit area. Docs Code dd-manage-deposit https://github.com/DANS-KNAW/dd-manage-deposit dd-manage-deposit-api https://github.com/DANS-KNAW/dd-manage-deposit-api dd-manage-deposit-cli https://github.com/DANS-KNAW/dd-manage-deposit-cli API and CLI not yet implemented The current implementation of dd-manage-deposit does not yet implement dd-manage-deposit-api and dd-manage-deposit-cli , but instead has a manually coded API and CLI. dd-sword2 \u00b6 DANS implementation of the SWORD v2 protocol for automated deposits. Docs Code dd-sword2 https://github.com/DANS-KNAW/dd-sword2 dd-dans-sword2-examples https://github.com/DANS-KNAW/dd-dans-sword2-examples dd-transfer-to-vault \u00b6 Service for preparing Dataset Version Exports for storage in the DANS Data Vault . This includes validation, registering the NBN for a dataset with the BRI-GMH resolver , creating a vault catalog entry and handing over the bag to the dd-data-vault service. Docs Code dd-transfer-to-vault https://github.com/DANS-KNAW/dd-transfer-to-vault dd-validate-bagpack \u00b6 Service that checks whether a bag complies with the BagPack specification and the DANS BagPack Profile. This service is used by dd-transfer-to-vault to make sure that the Dataset Version Exports (DVE) that are uploaded to the DANS Data Vault are valid. Docs Code dd-validate-bagback https://github.com/DANS-KNAW/dd-validate-bagback dans-bagpack-profile https://github.com/DANS-KNAW/dans-bagpack-profile NOT YET IMPLEMENTED This service is not yet implemented. dd-validate-dans-bag \u00b6 Service that checks whether a bag complies with DANS BagIt Profile v1. It is used by dd-dataverse-ingest and dd-vault-ingest to validate bags that are uploaded via dd-sword2 . Docs Code dd-validate-dans-bag https://github.com/DANS-KNAW/dd-validate-dans-bag dd-validate-dans-bag-api https://github.com/DANS-KNAW/dd-validate-dans-bag-api dd-validate-dans-bag-cli https://github.com/DANS-KNAW/dd-validate-dans-bag-cli DANS BagIt Profile v1 https://github.com/DANS-KNAW/dans-bagit-profile DANS schema https://github.com/DANS-KNAW/dans-schema dd-vault-catalog \u00b6 Service that manages a catalog of all Dataset Version Exports in the DANS Data Vault . It exposes a summary page for each stored dataset. Docs Code dd-vault-catalog https://github.com/DANS-KNAW/dd-vault-catalog dd-vault-catalog-api https://github.com/DANS-KNAW/dd-vault-catalog-api dd-vault-catalog-cli https://github.com/DANS-KNAW/dd-vault-catalog-cli dd-vault-ingest \u00b6 Service for converting deposits coming in via Vault as a Service to BagPacks. Docs Code dd-vault-ingest https://github.com/DANS-KNAW/dd-vault-ingest dd-vault-metadata \u00b6 A service that fills in the Vault Metadata block for a dataset version in Datavers. These metadata are by dd-transfer-to-vault to catalogue the long-term preservation copy of the dataset version when it is stored on tape. Docs Code dd-vault-metadata https://github.com/DANS-KNAW/dd-vault-metadata dd-vault-metadata-api https://github.com/DANS-KNAW/dd-vault-metadata-api Vault Metadata block N/A API not yet implemented The current implementation of dd-vault-metadata does not yet implement dd-vault-metadata-api , but instead has a manually coded API. dd-virus-scan \u00b6 A service that scans all files in a dataset for virus using clamav and blocks publication if a virus is found. Docs Code dd-virus-scan https://github.com/DANS-KNAW/dd-virus-scan dd-virus-scan-api https://github.com/DANS-KNAW/dd-virus-scan-api dans-datastation-tools (deprecated) \u00b6 Command line utilities for Data Station application management. Docs Code dans-datastation-tools https://github.com/DANS-KNAW/dans-datastation-tools","title":"DANS Microservices & Tools"},{"location":"dans-microservices/#dans-microservices-tools","text":"Several of the components in the architecture are implemented as microservices and developed by DANS. Microservice projects often have an associated API project, containing the OpenAPI specification as well as a Command Line Interface (CLI) project that provides a command line tool for interacting with the microservice. API versioning The head of the API project is not necessarily the version implemented by the microservice, but rather the latest version of the OpenAPI specification.","title":"DANS Microservices &amp; Tools"},{"location":"dans-microservices/#dd-confirm-archiving","text":"Monitors dataset version exports that are being archived by dd-data-vault and registers the archival timestamp in the Vault Catalog once the export is archived in the DANS Data Vault. Docs Code dd-confirm-archiving https://github.com/DANS-KNAW/dd-confirm-archiving dd-confirm-archiving-api https://github.com/DANS-KNAW/dd-confirm-archiving-api","title":"dd-confirm-archiving"},{"location":"dans-microservices/#dd-data-vault","text":"Service that manages a Data Vault Storage Root and provides a REST API for interacting with it. Docs Code dd-data-vault https://github.com/DANS-KNAW/dd-data-vault dd-data-vault-api https://github.com/DANS-KNAW/dd-data-vault-api dd-data-vault-cli https://github.com/DANS-KNAW/dd-data-vault-cli","title":"dd-data-vault"},{"location":"dans-microservices/#dd-dataverse-authenticator","text":"A proxy that authenticates clients on behalf of Dataverse, using the basic auth protocol or a Dataverse API token. It is used by dd-sword2 to authenticate its clients by their Dataverse account credentials. Docs Code dd-dataverse-authenticator https://github.com/DANS-KNAW/dd-dataverse-authenticator","title":"dd-dataverse-authenticator"},{"location":"dans-microservices/#dd-dataverse-cli","text":"Command line tool for interacting with the Dataverse service. It uses dans-dataverse-client-lib to communicate with the Dataverse API. Docs Code dd-dataverse-cli https://github.com/DANS-KNAW/dd-dataverse-cli Replacement for dans-datastation-tools This will replace most of the old dans-datastation-tools command line tools, which are now deprecated.","title":"dd-dataverse-cli"},{"location":"dans-microservices/#dd-dataverse-ingest","text":"Service for ingesting deposit directories into Dataverse. Docs Code dd-dataverse-ingest https://github.com/DANS-KNAW/dd-dataverse-ingest dd-dataverse-ingest-api https://github.com/DANS-KNAW/dd-dataverse-ingest-api dd-dataverse-ingest-cli https://github.com/DANS-KNAW/dd-dataverse-ingest-cli","title":"dd-dataverse-ingest"},{"location":"dans-microservices/#dd-gmh","text":"Command line tool for interacting with the BRI-GMH resolver to resolve a DANS Persistent Identifier (NBN) to a URL. Docs Code dd-gmh-api https://github.com/DANS-KNAW/dd-gmh-api In code https://persistent-identifier.nl/gmh-registration-service/api/v1/openapi.yaml dd-gmh-cli https://github.com/DANS-KNAW/dd-gmh-cli OpenAPI specification The dd-gmh-api project contains a copy of the OpenAPI specification to be used in the build process of the dd-gmh-cli project. Note, however, that the GMH used to be maintained by DANS, but is currently maintained and hosted by Seecr on behalf of the Koninklijke Bibliotheek (KB). The authoritative OpenAPI specification is published at https://persistent-identifier.nl/gmh-registration-service/api/v1/openapi.yaml .","title":"dd-gmh"},{"location":"dans-microservices/#dd-manage-deposit","text":"Service that manages and maintains information about deposits in a deposit area. Docs Code dd-manage-deposit https://github.com/DANS-KNAW/dd-manage-deposit dd-manage-deposit-api https://github.com/DANS-KNAW/dd-manage-deposit-api dd-manage-deposit-cli https://github.com/DANS-KNAW/dd-manage-deposit-cli API and CLI not yet implemented The current implementation of dd-manage-deposit does not yet implement dd-manage-deposit-api and dd-manage-deposit-cli , but instead has a manually coded API and CLI.","title":"dd-manage-deposit"},{"location":"dans-microservices/#dd-sword2","text":"DANS implementation of the SWORD v2 protocol for automated deposits. Docs Code dd-sword2 https://github.com/DANS-KNAW/dd-sword2 dd-dans-sword2-examples https://github.com/DANS-KNAW/dd-dans-sword2-examples","title":"dd-sword2"},{"location":"dans-microservices/#dd-transfer-to-vault","text":"Service for preparing Dataset Version Exports for storage in the DANS Data Vault . This includes validation, registering the NBN for a dataset with the BRI-GMH resolver , creating a vault catalog entry and handing over the bag to the dd-data-vault service. Docs Code dd-transfer-to-vault https://github.com/DANS-KNAW/dd-transfer-to-vault","title":"dd-transfer-to-vault"},{"location":"dans-microservices/#dd-validate-bagpack","text":"Service that checks whether a bag complies with the BagPack specification and the DANS BagPack Profile. This service is used by dd-transfer-to-vault to make sure that the Dataset Version Exports (DVE) that are uploaded to the DANS Data Vault are valid. Docs Code dd-validate-bagback https://github.com/DANS-KNAW/dd-validate-bagback dans-bagpack-profile https://github.com/DANS-KNAW/dans-bagpack-profile NOT YET IMPLEMENTED This service is not yet implemented.","title":"dd-validate-bagpack"},{"location":"dans-microservices/#dd-validate-dans-bag","text":"Service that checks whether a bag complies with DANS BagIt Profile v1. It is used by dd-dataverse-ingest and dd-vault-ingest to validate bags that are uploaded via dd-sword2 . Docs Code dd-validate-dans-bag https://github.com/DANS-KNAW/dd-validate-dans-bag dd-validate-dans-bag-api https://github.com/DANS-KNAW/dd-validate-dans-bag-api dd-validate-dans-bag-cli https://github.com/DANS-KNAW/dd-validate-dans-bag-cli DANS BagIt Profile v1 https://github.com/DANS-KNAW/dans-bagit-profile DANS schema https://github.com/DANS-KNAW/dans-schema","title":"dd-validate-dans-bag"},{"location":"dans-microservices/#dd-vault-catalog","text":"Service that manages a catalog of all Dataset Version Exports in the DANS Data Vault . It exposes a summary page for each stored dataset. Docs Code dd-vault-catalog https://github.com/DANS-KNAW/dd-vault-catalog dd-vault-catalog-api https://github.com/DANS-KNAW/dd-vault-catalog-api dd-vault-catalog-cli https://github.com/DANS-KNAW/dd-vault-catalog-cli","title":"dd-vault-catalog"},{"location":"dans-microservices/#dd-vault-ingest","text":"Service for converting deposits coming in via Vault as a Service to BagPacks. Docs Code dd-vault-ingest https://github.com/DANS-KNAW/dd-vault-ingest","title":"dd-vault-ingest"},{"location":"dans-microservices/#dd-vault-metadata","text":"A service that fills in the Vault Metadata block for a dataset version in Datavers. These metadata are by dd-transfer-to-vault to catalogue the long-term preservation copy of the dataset version when it is stored on tape. Docs Code dd-vault-metadata https://github.com/DANS-KNAW/dd-vault-metadata dd-vault-metadata-api https://github.com/DANS-KNAW/dd-vault-metadata-api Vault Metadata block N/A API not yet implemented The current implementation of dd-vault-metadata does not yet implement dd-vault-metadata-api , but instead has a manually coded API.","title":"dd-vault-metadata"},{"location":"dans-microservices/#dd-virus-scan","text":"A service that scans all files in a dataset for virus using clamav and blocks publication if a virus is found. Docs Code dd-virus-scan https://github.com/DANS-KNAW/dd-virus-scan dd-virus-scan-api https://github.com/DANS-KNAW/dd-virus-scan-api","title":"dd-virus-scan"},{"location":"dans-microservices/#dans-datastation-tools-deprecated","text":"Command line utilities for Data Station application management. Docs Code dans-datastation-tools https://github.com/DANS-KNAW/dans-datastation-tools","title":"dans-datastation-tools (deprecated)"},{"location":"data-archive/","text":"SURF Data Archive \u00b6 Data Archive is a tape storage facility provided by SURF , the collaborative ICT organization for Dutch education and research. It is used by the DANS Data Vault service to store its finished layers. The interface to Data Archive is provided by the dmftar utility, which is provided by SURF as open source software. See Data Vault Storage Root for more information about the DANS Data Vault layers.","title":"Data Archive (SURF)"},{"location":"data-archive/#surf-data-archive","text":"Data Archive is a tape storage facility provided by SURF , the collaborative ICT organization for Dutch education and research. It is used by the DANS Data Vault service to store its finished layers. The interface to Data Archive is provided by the dmftar utility, which is provided by SURF as open source software. See Data Vault Storage Root for more information about the DANS Data Vault layers.","title":"SURF Data Archive"},{"location":"data-vault-storage-root/","text":"Data Vault Storage Root \u00b6 Introduction \u00b6 The Data Vault is subdivided into Storage Roots , each one containing the long term preservation copies for either a Data Station or a \"Vault as a Service\" (VaaS) customer. The Data Vault Storage Root (DVSR) can be viewed as a type of interface, or exchange format, albeit an atypical one, as it is aimed at future users, rather than current ones. dd-data-vault interface Do not confuse the DVSR with the service interface of dd-data-vault , which is an internal microservice interface that is used by the transfer service to store data in the Data Vault. OCFL repositories \u00b6 The DANS Data Vault is implemented as an array of OCFL repositories. OCFL stands for Oxford Common File Layout . It is a community specification for the layout of a repository that stores versioned digital objects. Each repository, or \"storage root,\" is one Data Vault Storage Root (DVSR) . The Data Stations each have their own DVSR as does each customer of the Vault as a Service. Extensions \u00b6 OCFL can be extended with additional metadata and functionality. The DANS Data Vault uses the following extensions: OCFL Packaging Format extension - This extension defines a way to specify for each version of an object how it is packaged. Extension for supporting deletion. TODO . Dataset model mapping \u00b6 OCFL is a generic storage model. It does not define the concept of a dataset. The DANS archival systems (Data Stations and Vault as a Service), on the other hand, are built around the dataset concept. The mapping between the two models is as follows: DANS dataset model OCFL model Dataset OCFL Object Dataset Version OCFL Object Version Datafile OCFL Content File Versions \u00b6 Each Dataset Version Export (DVE) is stored in a separate OCFL Object Version. This means that there is a 1-to-1 mapping between a DVE and an OCFL Object Version. Note however, that it is possible that one dataset version is exported multiple times. The mapping of a dataset version to an OCFL Object is therefore a 1-to- n relationship. A multiple exports scenario A scenario where a dataset version is exported multiple times is when the dataset was updated in the Data Station without creating a new version. This can be done by a superuser and is known as \"updatecurrent\" . A new Dataset Version Export will be created and therefore a new OCFL Object Version will be created as well. The Data Station version history, however, will not display an additional version. Identifying metadata \u00b6 To identify datasets, versions and data files in the OCFL repository, the following metadata is used: The full metadata of each dataset version is stored, but the way it is stored depends on the packaging format used. The current packaging format is based on Dataverse implementation of RDA BagPack . Serialization in layers \u00b6 OCFL repositories can be serialized in different ways, for example as a directory structure on a file system, or as objects in an object store. The DANS Data Vault uses the SURF Data Archive tape storage. The tape storage system that is used by Data Archive organizes files in a file-folder structure, so in principle serialization should be the same as to a disk-based files system, from OCFL's perspective. However, the tape storage system requires a minimum file size of 1GB, which is much larger than the typical data file stored in the DANS Data Vault. To meet this requirement, the OCFL repositories are stored as a series of DMFTAR archives, each of which is larger than 1GB. Each archive forms a layer. For a more detailed description of the layers, see the documentation of dans-layer-store-lib . To restore the OCFL repository, the layers must be extracted in the correct order. SURF provides a utility called dmftar to create and extract DMFTAR archives. This utility is the interface to the tape storage system. Restoring without the dmftar utility Even without the dmftar utility, it is possible to restore the OCFL repository, as long as the layers are extracted in the correct order. A DMFTAR archive is just a lightweight wrapper around a TAR archive, implemented as a directory containing batches of (possibly multi-volume) TAR files along with index files and a checksum file.","title":"Data Vault Storage Root"},{"location":"data-vault-storage-root/#data-vault-storage-root","text":"","title":"Data Vault Storage Root"},{"location":"data-vault-storage-root/#introduction","text":"The Data Vault is subdivided into Storage Roots , each one containing the long term preservation copies for either a Data Station or a \"Vault as a Service\" (VaaS) customer. The Data Vault Storage Root (DVSR) can be viewed as a type of interface, or exchange format, albeit an atypical one, as it is aimed at future users, rather than current ones. dd-data-vault interface Do not confuse the DVSR with the service interface of dd-data-vault , which is an internal microservice interface that is used by the transfer service to store data in the Data Vault.","title":"Introduction"},{"location":"data-vault-storage-root/#ocfl-repositories","text":"The DANS Data Vault is implemented as an array of OCFL repositories. OCFL stands for Oxford Common File Layout . It is a community specification for the layout of a repository that stores versioned digital objects. Each repository, or \"storage root,\" is one Data Vault Storage Root (DVSR) . The Data Stations each have their own DVSR as does each customer of the Vault as a Service.","title":"OCFL repositories"},{"location":"data-vault-storage-root/#extensions","text":"OCFL can be extended with additional metadata and functionality. The DANS Data Vault uses the following extensions: OCFL Packaging Format extension - This extension defines a way to specify for each version of an object how it is packaged. Extension for supporting deletion. TODO .","title":"Extensions"},{"location":"data-vault-storage-root/#dataset-model-mapping","text":"OCFL is a generic storage model. It does not define the concept of a dataset. The DANS archival systems (Data Stations and Vault as a Service), on the other hand, are built around the dataset concept. The mapping between the two models is as follows: DANS dataset model OCFL model Dataset OCFL Object Dataset Version OCFL Object Version Datafile OCFL Content File","title":"Dataset model mapping"},{"location":"data-vault-storage-root/#versions","text":"Each Dataset Version Export (DVE) is stored in a separate OCFL Object Version. This means that there is a 1-to-1 mapping between a DVE and an OCFL Object Version. Note however, that it is possible that one dataset version is exported multiple times. The mapping of a dataset version to an OCFL Object is therefore a 1-to- n relationship. A multiple exports scenario A scenario where a dataset version is exported multiple times is when the dataset was updated in the Data Station without creating a new version. This can be done by a superuser and is known as \"updatecurrent\" . A new Dataset Version Export will be created and therefore a new OCFL Object Version will be created as well. The Data Station version history, however, will not display an additional version.","title":"Versions"},{"location":"data-vault-storage-root/#identifying-metadata","text":"To identify datasets, versions and data files in the OCFL repository, the following metadata is used: The full metadata of each dataset version is stored, but the way it is stored depends on the packaging format used. The current packaging format is based on Dataverse implementation of RDA BagPack .","title":"Identifying metadata"},{"location":"data-vault-storage-root/#serialization-in-layers","text":"OCFL repositories can be serialized in different ways, for example as a directory structure on a file system, or as objects in an object store. The DANS Data Vault uses the SURF Data Archive tape storage. The tape storage system that is used by Data Archive organizes files in a file-folder structure, so in principle serialization should be the same as to a disk-based files system, from OCFL's perspective. However, the tape storage system requires a minimum file size of 1GB, which is much larger than the typical data file stored in the DANS Data Vault. To meet this requirement, the OCFL repositories are stored as a series of DMFTAR archives, each of which is larger than 1GB. Each archive forms a layer. For a more detailed description of the layers, see the documentation of dans-layer-store-lib . To restore the OCFL repository, the layers must be extracted in the correct order. SURF provides a utility called dmftar to create and extract DMFTAR archives. This utility is the interface to the tape storage system. Restoring without the dmftar utility Even without the dmftar utility, it is possible to restore the OCFL repository, as long as the layers are extracted in the correct order. A DMFTAR archive is just a lightweight wrapper around a TAR archive, implemented as a directory containing batches of (possibly multi-volume) TAR files along with index files and a checksum file.","title":"Serialization in layers"},{"location":"datastation/","text":"Data Station architecture \u00b6 Overview \u00b6 This document gives an overview of the Data Station architecture. The schema below displays the components of a Data Station and how they relate to each other. The notation used is not a formal one and is intended to be self-explanatory. To the extent that it is not, you might want to consult the legend that is included at the end of this page . Enlarge Image Actors \u00b6 Data Station User - a user of the Data Station, typically a customer who downloads or deposits data. Data Manager - a user with special privileges, who curates and publishes datasets submitted for review by a user. SWORD2 Client - a software client that interacts with the DANS SWORD2 Service . Schema Legend \u00b6 Enlarge Image","title":"Data Station"},{"location":"datastation/#data-station-architecture","text":"","title":"Data Station architecture"},{"location":"datastation/#overview","text":"This document gives an overview of the Data Station architecture. The schema below displays the components of a Data Station and how they relate to each other. The notation used is not a formal one and is intended to be self-explanatory. To the extent that it is not, you might want to consult the legend that is included at the end of this page . Enlarge Image","title":"Overview"},{"location":"datastation/#actors","text":"Data Station User - a user of the Data Station, typically a customer who downloads or deposits data. Data Manager - a user with special privileges, who curates and publishes datasets submitted for review by a user. SWORD2 Client - a software client that interacts with the DANS SWORD2 Service .","title":"Actors"},{"location":"datastation/#schema-legend","text":"Enlarge Image","title":"Schema Legend"},{"location":"dataverse/","text":"Dataverse \u00b6 \"The Dataverse Project is an open source web application to share, preserve, cite, explore, and analyze research data.\" In the Data Station this repository system is used for depositing, storing and disseminating datasets, as well as creating long-term preservation copies of those datasets. Workflows \u00b6 Dataverse provides event hooks that allow to configure workflows to run just before and after a publication event. These workflows can have multiple steps. A step can be implemented as part of Dataverse or as an external service. The following microservices are configured to run as PrePublishDataset workflow steps: dd-vault-metadata The following microservices are candidates to become part of the PrePublishDataset workflow in the future: dd-virus-scan The BagPack export flow step is implemented in Dataverse and is used to create an export of the published dataset version (a \"Dataset Version Export\" or DVE). The export complies with the RDA's BagPack recommendation . This exported bag is then picked up by dd-transfer-to-vault . Docs Code Dataverse https://github.com/IQSS/dataverse Workflows Part of the Dataverse code base","title":"Dataverse"},{"location":"dataverse/#dataverse","text":"\"The Dataverse Project is an open source web application to share, preserve, cite, explore, and analyze research data.\" In the Data Station this repository system is used for depositing, storing and disseminating datasets, as well as creating long-term preservation copies of those datasets.","title":"Dataverse"},{"location":"dataverse/#workflows","text":"Dataverse provides event hooks that allow to configure workflows to run just before and after a publication event. These workflows can have multiple steps. A step can be implemented as part of Dataverse or as an external service. The following microservices are configured to run as PrePublishDataset workflow steps: dd-vault-metadata The following microservices are candidates to become part of the PrePublishDataset workflow in the future: dd-virus-scan The BagPack export flow step is implemented in Dataverse and is used to create an export of the published dataset version (a \"Dataset Version Export\" or DVE). The export complies with the RDA's BagPack recommendation . This exported bag is then picked up by dd-transfer-to-vault . Docs Code Dataverse https://github.com/IQSS/dataverse Workflows Part of the Dataverse code base","title":"Workflows"},{"location":"dataversenl/","text":"DataverseNL \u00b6 The differences between DataverseNL and the Data Stations are mainly contractual and organisational. In DataverseNL, the customer organization is responsible for the curation of datasets. The technical differences are minimal: DataverseNL has been configured with multiple collections, each of which is administered by a customer organization. The Data Stations have a single collection per Data Station, which is administered by DANS. DataverseNL has no SWORD2 service.","title":"DataverseNL"},{"location":"dataversenl/#dataversenl","text":"The differences between DataverseNL and the Data Stations are mainly contractual and organisational. In DataverseNL, the customer organization is responsible for the curation of datasets. The technical differences are minimal: DataverseNL has been configured with multiple collections, each of which is administered by a customer organization. The Data Stations have a single collection per Data Station, which is administered by DANS. DataverseNL has no SWORD2 service.","title":"DataverseNL"},{"location":"deposit-directory/","text":"Deposit directory \u00b6 A deposit directory is a directory containing: deposit files deposit properties . \u2514\u2500\u2500 deposit-directory \u251c\u2500\u2500 <deposit files> \u2514\u2500\u2500 deposit.properties <deposit files> \u00b6 The deposit files are one or more files or directories. Typically, it will be one directory, a bag , and more specifically, one conforming to the DANS BagIt Profile . However, applications have different requirements with respect to the contents and lay-out of the deposit. deposit.properties \u00b6 Processing metadata about the deposit are stored in a properties file called deposit.properties . It shall have at minimum the following properties: Key Format Description creation.timestamp ISO 8601 datetime, including timezone and in ms precision Date/time when the deposit directory was created state.label A label indicating the current state of the deposit state.description A human readable description of the state or an error message, if state.label indicates an error Applications may use additional properties.","title":"Deposit Directory"},{"location":"deposit-directory/#deposit-directory","text":"A deposit directory is a directory containing: deposit files deposit properties . \u2514\u2500\u2500 deposit-directory \u251c\u2500\u2500 <deposit files> \u2514\u2500\u2500 deposit.properties","title":"Deposit directory"},{"location":"deposit-directory/#deposit-files","text":"The deposit files are one or more files or directories. Typically, it will be one directory, a bag , and more specifically, one conforming to the DANS BagIt Profile . However, applications have different requirements with respect to the contents and lay-out of the deposit.","title":"&lt;deposit files&gt;"},{"location":"deposit-directory/#depositproperties","text":"Processing metadata about the deposit are stored in a properties file called deposit.properties . It shall have at minimum the following properties: Key Format Description creation.timestamp ISO 8601 datetime, including timezone and in ms precision Date/time when the deposit directory was created state.label A label indicating the current state of the deposit state.description A human readable description of the state or an error message, if state.label indicates an error Applications may use additional properties.","title":"deposit.properties"},{"location":"dev-common-practices/","text":"Common Practices \u00b6 Here are some miscellaneous common practices to follow when developing for the DANS Data Station Architecture. It is assumed that you are using IntelliJ. If this is not the case, please adapt accordingly. Code Style \u00b6 Install the IntelliJ code style and inspections . Format the source code with these settings in our own projects. Resolve the warnings indicated by the inspections. In Dataverse try to minimize code changes due to reformatting, so only apply formatting to code that you have changed anyway. In the POM file keep the order the of the elements as follows (note that some are optional): modelVersion parent groupId artifactId version name url description inceptionYear properties scm dependencyManagement dependencies build repositories distributionManagement profiles Lombok \u00b6 Use Lombok for: Adding loggers with the @Slf4j annotation (this names the logger after the fully qualified class name automatically). Creating getters, setters and constructors on value objects (i.e. the main purpose of the object is to store values and not to perform operations). Dependency Management \u00b6 Dependency management of Java projects is done with Maven. Projects should inherit from dd-parent . The first thing that most projects will include is io.dropwizard:dropwizard-core . Testing \u00b6 Unit test names should be as descriptive as possible. Since this will often involve creating long test names, use snake_case instead of camelCase here. Unit tests will often need to write some temporary data to disk. The location for this is <project-dir>/target/test/<ClassNameOfUnitTest> . By working under target we make sure the unit tests don't interfere with the project itself or the test files we are using for debugging . Unit tests should clear their temporary directory before the tests start, but leave everything on disk after finishing. This allows you to diagnose any problems with a test by running it and inspecting its temporary directory. Debugging \u00b6 For debugging use the scripts in dans-dev-scripts . Start the environment with start-env.sh . This sets up configuration files under <project-dir>/etc/ for use during debugging and create directory to store input and output files under <project-dir>/data/ . Start the program with the start-*debug.sh helper scripts and then attach IntelliJ to the VM. Documentation \u00b6 Each module has its associated documentation site, which is published at dans-knaw.github.io . The archetype sets up the project with a skeleton site. Use the start-mkdocs.sh script in dans-dev-scripts to start the site locally and see what it looks like after you have made your changes. Each documentation site follows a common structure which depends on the type of module: library, microservice, or command-line application. See existing documentation sites for examples. JavaDocs \u00b6 If the project is a library it should include JavaDocs. Extensive code examples are best relegated to separated pages, outside the JavaDocs, so that you can make full use of the extended Markdown support of mkdocs. You can then link to those from the JavaDocs.","title":"Common practices"},{"location":"dev-common-practices/#common-practices","text":"Here are some miscellaneous common practices to follow when developing for the DANS Data Station Architecture. It is assumed that you are using IntelliJ. If this is not the case, please adapt accordingly.","title":"Common Practices"},{"location":"dev-common-practices/#code-style","text":"Install the IntelliJ code style and inspections . Format the source code with these settings in our own projects. Resolve the warnings indicated by the inspections. In Dataverse try to minimize code changes due to reformatting, so only apply formatting to code that you have changed anyway. In the POM file keep the order the of the elements as follows (note that some are optional): modelVersion parent groupId artifactId version name url description inceptionYear properties scm dependencyManagement dependencies build repositories distributionManagement profiles","title":"Code Style"},{"location":"dev-common-practices/#lombok","text":"Use Lombok for: Adding loggers with the @Slf4j annotation (this names the logger after the fully qualified class name automatically). Creating getters, setters and constructors on value objects (i.e. the main purpose of the object is to store values and not to perform operations).","title":"Lombok"},{"location":"dev-common-practices/#dependency-management","text":"Dependency management of Java projects is done with Maven. Projects should inherit from dd-parent . The first thing that most projects will include is io.dropwizard:dropwizard-core .","title":"Dependency Management"},{"location":"dev-common-practices/#testing","text":"Unit test names should be as descriptive as possible. Since this will often involve creating long test names, use snake_case instead of camelCase here. Unit tests will often need to write some temporary data to disk. The location for this is <project-dir>/target/test/<ClassNameOfUnitTest> . By working under target we make sure the unit tests don't interfere with the project itself or the test files we are using for debugging . Unit tests should clear their temporary directory before the tests start, but leave everything on disk after finishing. This allows you to diagnose any problems with a test by running it and inspecting its temporary directory.","title":"Testing"},{"location":"dev-common-practices/#debugging","text":"For debugging use the scripts in dans-dev-scripts . Start the environment with start-env.sh . This sets up configuration files under <project-dir>/etc/ for use during debugging and create directory to store input and output files under <project-dir>/data/ . Start the program with the start-*debug.sh helper scripts and then attach IntelliJ to the VM.","title":"Debugging"},{"location":"dev-common-practices/#documentation","text":"Each module has its associated documentation site, which is published at dans-knaw.github.io . The archetype sets up the project with a skeleton site. Use the start-mkdocs.sh script in dans-dev-scripts to start the site locally and see what it looks like after you have made your changes. Each documentation site follows a common structure which depends on the type of module: library, microservice, or command-line application. See existing documentation sites for examples.","title":"Documentation"},{"location":"dev-common-practices/#javadocs","text":"If the project is a library it should include JavaDocs. Extensive code examples are best relegated to separated pages, outside the JavaDocs, so that you can make full use of the extended Markdown support of mkdocs. You can then link to those from the JavaDocs.","title":"JavaDocs"},{"location":"dev/","text":"Development \u00b6 The following sections discuss development on various parts of a Data Station. Some components are developed by DANS, other components are provided by open source projects in which DANS may participate. DANS microservices \u00b6 The DANS microservices are based on the dans-module-archetype . This archetype creates a skeleton microservice based on the DropWizard framework. When working on DANS microservices, please comply with the best practises documented in the dans-module-archetype documentation. DANS command-line tools \u00b6 DANS command line tools are written in Java as well. They are based on the dans-cli-archetype . This archetype creates a skeleton command line tool which uses some of the DropWizard facilities. Python command line tools Previously DANS command line tools were written in Python, but his approach is being phased out. Dataverse \u00b6 The architecture overview makes clear that Dataverse plays a key role in the Data Station. That is why DANS is actively involved in its development via the Dataverse community. When working on Dataverse code, take notice of the developer docs . Debugging Dataverse For DANS developers it is not necessary (nor preferable) to set up Dataverse and its dependencies Solr and PostGreSQL on your development laptop, as described in the developer docs. Instead you should use the pre-built vagrant boxes, which are available only for DANS developers. Skosmos \u00b6 Skosmos is used to serve vocabulary terms to Dataverse. DANS is currently not actively involved in development, but it is entirely possible that bug fixes may need to be contributed in the future. The project is written in PHP but there is no information on the Skosmos website about the development environment set-up. Documentation with mkdocs \u00b6 The documentation for DANS projects (including this site) is written using mkdocs . The source code for those sites consists of markdown in combination with other resources, such as images. Images are often created with yEd . The graphml source code of the images is committed along with the image exports. You should always check that your changes render correctly. This is made easy by the start-mkdocs.sh script in the dans-core-systems project (only available for DANS developers).","title":"Overview"},{"location":"dev/#development","text":"The following sections discuss development on various parts of a Data Station. Some components are developed by DANS, other components are provided by open source projects in which DANS may participate.","title":"Development"},{"location":"dev/#dans-microservices","text":"The DANS microservices are based on the dans-module-archetype . This archetype creates a skeleton microservice based on the DropWizard framework. When working on DANS microservices, please comply with the best practises documented in the dans-module-archetype documentation.","title":"DANS microservices"},{"location":"dev/#dans-command-line-tools","text":"DANS command line tools are written in Java as well. They are based on the dans-cli-archetype . This archetype creates a skeleton command line tool which uses some of the DropWizard facilities. Python command line tools Previously DANS command line tools were written in Python, but his approach is being phased out.","title":"DANS command-line tools"},{"location":"dev/#dataverse","text":"The architecture overview makes clear that Dataverse plays a key role in the Data Station. That is why DANS is actively involved in its development via the Dataverse community. When working on Dataverse code, take notice of the developer docs . Debugging Dataverse For DANS developers it is not necessary (nor preferable) to set up Dataverse and its dependencies Solr and PostGreSQL on your development laptop, as described in the developer docs. Instead you should use the pre-built vagrant boxes, which are available only for DANS developers.","title":"Dataverse"},{"location":"dev/#skosmos","text":"Skosmos is used to serve vocabulary terms to Dataverse. DANS is currently not actively involved in development, but it is entirely possible that bug fixes may need to be contributed in the future. The project is written in PHP but there is no information on the Skosmos website about the development environment set-up.","title":"Skosmos"},{"location":"dev/#documentation-with-mkdocs","text":"The documentation for DANS projects (including this site) is written using mkdocs . The source code for those sites consists of markdown in combination with other resources, such as images. Images are often created with yEd . The graphml source code of the images is committed along with the image exports. You should always check that your changes render correctly. This is made easy by the start-mkdocs.sh script in the dans-core-systems project (only available for DANS developers).","title":"Documentation with mkdocs"},{"location":"external-interfaces/","text":"External interfaces \u00b6 The following interfaces are exposed to the outside world: The Dataverse user interface . This interface used by human users. The Dataverse RESTful HTTP API . This interface is used by internal and external systems that need to interact with Dataverse. A SWORD2 deposit service . This interface is used by external systems that want to deposit data in Dataverse or directly in the Data Vault. Long term preservation copies of the data via the Data Vault Storage Root . This interface (or exchange format) is intended for future users who need to disseminate the data stored in the Data Vault after the current dissemination infrastructure has been decommissioned.","title":"Overview"},{"location":"external-interfaces/#external-interfaces","text":"The following interfaces are exposed to the outside world: The Dataverse user interface . This interface used by human users. The Dataverse RESTful HTTP API . This interface is used by internal and external systems that need to interact with Dataverse. A SWORD2 deposit service . This interface is used by external systems that want to deposit data in Dataverse or directly in the Data Vault. Long term preservation copies of the data via the Data Vault Storage Root . This interface (or exchange format) is intended for future users who need to disseminate the data stored in the Data Vault after the current dissemination infrastructure has been decommissioned.","title":"External interfaces"},{"location":"internal-interfaces/","text":"Overview \u00b6 The Data Station architecture follows the microservices architectural style as far as possible. Dataverse is the only component that is not a microservice. It is a monolith that is used as a black box. The interfaces between the microservices fall into the following categories: RESTful HTTP interfaces. These interfaces are described in the documentation page of the microservice. Shared disk space. In the automated deposit pipeline deposits are passed from one microservice to the next by storing them in a shared disk space. The interchange format that is used by several microservices is the Deposit Directory .","title":"Overview"},{"location":"internal-interfaces/#overview","text":"The Data Station architecture follows the microservices architectural style as far as possible. Dataverse is the only component that is not a microservice. It is a monolith that is used as a black box. The interfaces between the microservices fall into the following categories: RESTful HTTP interfaces. These interfaces are described in the documentation page of the microservice. Shared disk space. In the automated deposit pipeline deposits are passed from one microservice to the next by storing them in a shared disk space. The interchange format that is used by several microservices is the Deposit Directory .","title":"Overview"},{"location":"nbn-resolver/","text":"NBN Resolver \u00b6 The NBN resolver service is operated by the Koninklijke Bibliotheek . It resolves NBN persistent identifiers to their current location. The resolver is hosted at https://persistent-identifier.nl/ . The DANS Data Vault registers the long-term preservation copies of datasets with the NBN resolver through its API .","title":"NBN Resolver (KB)"},{"location":"nbn-resolver/#nbn-resolver","text":"The NBN resolver service is operated by the Koninklijke Bibliotheek . It resolves NBN persistent identifiers to their current location. The resolver is hosted at https://persistent-identifier.nl/ . The DANS Data Vault registers the long-term preservation copies of datasets with the NBN resolver through its API .","title":"NBN Resolver"},{"location":"object-store/","text":"SURF Object Store \u00b6 Object Store is a facility provided by SURF , the collaborative ICT organization for Dutch education and research. It is used by the Dataverse instances in the Data Stations and DataverseNL to store the data files of the datasets.","title":"Object Store (SURF)"},{"location":"object-store/#surf-object-store","text":"Object Store is a facility provided by SURF , the collaborative ICT organization for Dutch education and research. It is used by the Dataverse instances in the Data Stations and DataverseNL to store the data files of the datasets.","title":"SURF Object Store"},{"location":"skosmos/","text":"Skosmos \u00b6 A thesaurus service developed by the National Library of Finland. It is used to serve the external controlled vocabulary fields. Docs Code Skosmos https://github.com/NatLibFi/Skosmos","title":"Skosmos"},{"location":"skosmos/#skosmos","text":"A thesaurus service developed by the National Library of Finland. It is used to serve the external controlled vocabulary fields. Docs Code Skosmos https://github.com/NatLibFi/Skosmos","title":"Skosmos"},{"location":"status/","text":"Build status \u00b6 Component Build status Dataverse dans-api-archetype dans-bagit-lib dans-cli-archetype dans-converter-lib dans-dataverse-client-lib dans-java-utils dans-layer-store-lib dans-module-archetype dans-ocfl-java-extensions-lib dans-schema dans-validation-lib dd-dans-sword2-examples dd-data-vault dd-data-vault-api dd-data-vault-cli dd-dataverse-authenticator dd-dataverse-cli dd-dataverse-ingest dd-dataverse-ingest-api dd-dataverse-ingest-cli dd-gmh-api dd-gmh-cli dd-manage-deposit dd-manage-deposit-api dd-manage-deposit-cli dd-parent dd-sword2 dd-transfer-to-vault dd-validate-dans-bag dd-validate-dans-bag-api dd-validate-dans-bag-cli dd-vault-catalog dd-vault-catalog-api dd-vault-catalog-cli dd-vault-ingest dd-vault-metadata dd-vault-metadata-api dd-virus-scan dd-virus-scan-api easy-mirror-deposit","title":"Build status"},{"location":"status/#build-status","text":"Component Build status Dataverse dans-api-archetype dans-bagit-lib dans-cli-archetype dans-converter-lib dans-dataverse-client-lib dans-java-utils dans-layer-store-lib dans-module-archetype dans-ocfl-java-extensions-lib dans-schema dans-validation-lib dd-dans-sword2-examples dd-data-vault dd-data-vault-api dd-data-vault-cli dd-dataverse-authenticator dd-dataverse-cli dd-dataverse-ingest dd-dataverse-ingest-api dd-dataverse-ingest-cli dd-gmh-api dd-gmh-cli dd-manage-deposit dd-manage-deposit-api dd-manage-deposit-cli dd-parent dd-sword2 dd-transfer-to-vault dd-validate-dans-bag dd-validate-dans-bag-api dd-validate-dans-bag-cli dd-vault-catalog dd-vault-catalog-api dd-vault-catalog-cli dd-vault-ingest dd-vault-metadata dd-vault-metadata-api dd-virus-scan dd-virus-scan-api easy-mirror-deposit","title":"Build status"},{"location":"sword/","text":"SWORD \u00b6 The Simple Web-service Offering Repository Deposit (SWORD) protocol is a standard for depositing content into repositories. The DANS implementation of version 2 of this protocol has been operational for several years for depositing datasets in EASY . The SWORD2 interface is also available in the Data Stations. SWORD3 not yet available The latest version of SWORD is version 3. This version is not yet available in the Data Stations. Currently, we are focusing on guaranteeing the continued availability of the SWORD2 interface for our existing customers. It is not yet decided whether we will implement SWORD3 in the future. Guide and code examples for SWORD2 client developers \u00b6 The following documents and examples are available for developers who want to use the DANS SWORD2 service: Guide for DANS SWORD2 client developers Code examples DANS BagIt Profile \u00b6 The bags that are deposited to the DANS SWORD2 service must comply with the DANS BagIt Profile. DANS BagIt Profile","title":"SWORD"},{"location":"sword/#sword","text":"The Simple Web-service Offering Repository Deposit (SWORD) protocol is a standard for depositing content into repositories. The DANS implementation of version 2 of this protocol has been operational for several years for depositing datasets in EASY . The SWORD2 interface is also available in the Data Stations. SWORD3 not yet available The latest version of SWORD is version 3. This version is not yet available in the Data Stations. Currently, we are focusing on guaranteeing the continued availability of the SWORD2 interface for our existing customers. It is not yet decided whether we will implement SWORD3 in the future.","title":"SWORD"},{"location":"sword/#guide-and-code-examples-for-sword2-client-developers","text":"The following documents and examples are available for developers who want to use the DANS SWORD2 service: Guide for DANS SWORD2 client developers Code examples","title":"Guide and code examples for SWORD2 client developers"},{"location":"sword/#dans-bagit-profile","text":"The bags that are deposited to the DANS SWORD2 service must comply with the DANS BagIt Profile. DANS BagIt Profile","title":"DANS BagIt Profile"},{"location":"temporary-vault-config/","text":"Temporary Vault Configuration \u00b6 The DANS Data Vault is currently still in development. In the meantime, long term preservation is supported by storage of RDA bags in a Temporary Vault. The packaging of the dataset versions is therefore the same as for the DANS Data Vault, but the storage location is different and does not yet have an OCFL structure. Enlarge Image","title":"Temporary Vault"},{"location":"temporary-vault-config/#temporary-vault-configuration","text":"The DANS Data Vault is currently still in development. In the meantime, long term preservation is supported by storage of RDA bags in a Temporary Vault. The packaging of the dataset versions is therefore the same as for the DANS Data Vault, but the storage location is different and does not yet have an OCFL structure. Enlarge Image","title":"Temporary Vault Configuration"},{"location":"vaas/","text":"Vault as a Service \u00b6 Overview \u00b6 Clients can also use the DANS Data Vault as a building block in their own archival workflows. To this end DANS offers Vault as a Service . It exposes the same SWORD2 interface as the Data Stations. Instead of storing the datasets in Dataverse, they are directly converted into BagPacks and transferred to the Data Vault. In this scenario, curation as well as dissemination of the datasets remain the responsibility of the customer. Enlarge Image Components \u00b6 The \"Vault as a Service\" configuration has mostly the same components as a Data Station. The main difference is that Dataverse is not part of the configuration. Instead, a new component, dd-vault-ingest , is introduced to convert the datasets into RDA compliant bags and transfer them to the Data Vault.","title":"Vault as a Service"},{"location":"vaas/#vault-as-a-service","text":"","title":"Vault as a Service"},{"location":"vaas/#overview","text":"Clients can also use the DANS Data Vault as a building block in their own archival workflows. To this end DANS offers Vault as a Service . It exposes the same SWORD2 interface as the Data Stations. Instead of storing the datasets in Dataverse, they are directly converted into BagPacks and transferred to the Data Vault. In this scenario, curation as well as dissemination of the datasets remain the responsibility of the customer. Enlarge Image","title":"Overview"},{"location":"vaas/#components","text":"The \"Vault as a Service\" configuration has mostly the same components as a Data Station. The main difference is that Dataverse is not part of the configuration. Instead, a new component, dd-vault-ingest , is introduced to convert the datasets into RDA compliant bags and transfer them to the Data Vault.","title":"Components"},{"location":"vault-pipeline/","text":"Vault pipeline \u00b6 Each Data Station and each VaaS has its own Vault Pipeline instance. The diagram below illustrates the data flow through a Data Vault Pipeline. The data flow is displayed on three levels of abstraction from top to bottom:","title":"Data Vault pipeline"},{"location":"vault-pipeline/#vault-pipeline","text":"Each Data Station and each VaaS has its own Vault Pipeline instance. The diagram below illustrates the data flow through a Data Vault Pipeline. The data flow is displayed on three levels of abstraction from top to bottom:","title":"Vault pipeline"}]}